{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화 (Tokenize)\n",
    "- 문장을 의미있는 최소 단위로 나누는 것\n",
    "- 최소 단위 => 단어, 글자, 문장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 자모 단위 토큰화\n",
    "- 한글 문장을 한글 자모 단위로 나누어 토큰화\n",
    "- pip install jamo or conda install jamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'좋은 날'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jamo\n",
    "\n",
    "msg ='좋은 날'\n",
    "\n",
    "result1 = jamo.h2j(msg)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ㅈㅗㅎㅇㅡㄴ ㄴㅏㄹ'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = jamo.j2hcj(result1)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['좋은 날']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "\n",
    "nlp = konlpy.tag.Komoran()\n",
    "nlp.morphs(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ᄋ', 'ᅡ', 'ᆫ', 'ᄂ', 'ᅧ', 'ᆼ', 'ᄒ', 'ᅡ', 'ᄉ', 'ᅦ', 'ᄋ', 'ᅭ', '.', 'ᄇ', 'ᅡ', 'ᆫ', 'ᄀ', 'ᅡ', 'ᆸ', 'ᄉ', 'ᅳ', 'ᆸ', 'ᄂ', 'ᅵ', 'ᄃ', 'ᅡ', '.', 'ᄋ', 'ᅧ', 'ᄇ', 'ᅩ', 'ᄉ', 'ᅦ', 'ᄋ', 'ᅭ', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['안녕하세요', '.', '반갑습니다', '.', '여보세요', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # 한글 자모 분리\n",
    "    text = jamo.h2j(text)\n",
    "\n",
    "    # 한글 자모 단어 분리\n",
    "    words = text.split(' ')\n",
    "\n",
    "    # 한글 자모 단어 토큰화\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        # 한글 자모 단어 토큰화\n",
    "        tokens.extend(list(word))\n",
    "\n",
    "    return tokens\n",
    "\n",
    "text = \"안녕하세요. 반갑습니다. 여보세요.\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "nlp.morphs(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 다양한 한국어 형태소 분석기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okt\n",
      "명사 : ['오늘', '저녁', '실증', '랩', '성능', '테스트', '진행', '예정', '존나']\n",
      "형태소 : ['오늘', '저녁', '에', '실증', '랩', '에서', 'PC', '성능', '테스트', '를', '진행', '할', '예정', '입니다', '.', '존나', '맛있다']\n",
      "품사 : [('오늘', 'Noun'), ('저녁', 'Noun'), ('에', 'Josa'), ('실증', 'Noun'), ('랩', 'Noun'), ('에서', 'Josa'), ('PC', 'Alpha'), ('성능', 'Noun'), ('테스트', 'Noun'), ('를', 'Josa'), ('진행', 'Noun'), ('할', 'Verb'), ('예정', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation'), ('존나', 'Noun'), ('맛있다', 'Adjective')]\n",
      "Kkma\n",
      "명사 : ['오늘', '저녁', '실증', '실증랩', '랩', '성능', '테스트', '진행', '예정']\n",
      "형태소 : ['오늘', '저녁', '에', '실증', '랩', '에서', 'PC', '성능', '테스트', '를', '진행', '하', 'ㄹ', '예정', '이', 'ㅂ니다', '.', '존나', '맛있', '다']\n",
      "품사 : [('오늘', 'NNG'), ('저녁', 'NNG'), ('에', 'JKM'), ('실증', 'NNG'), ('랩', 'NNG'), ('에서', 'JKM'), ('PC', 'OL'), ('성능', 'NNG'), ('테스트', 'NNG'), ('를', 'JKO'), ('진행', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('예정', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EFN'), ('.', 'SF'), ('존나', 'MAG'), ('맛있', 'VA'), ('다', 'EFN')]\n",
      "Mecab\n",
      "명사 : ['저녁', '실증', '랩', '성능', '테스트', '예정']\n",
      "형태소 : ['오늘', '저녁', '에', '실증', '랩', '에서', 'PC', '성능', '테스트', '를', '진행할', '예정', '입니다', '.', '존나', '맛있', '다']\n",
      "품사 : [('오늘', 'MAG'), ('저녁', 'NNG'), ('에', 'JKB'), ('실증', 'NNG'), ('랩', 'NNG'), ('에서', 'JKB'), ('PC', 'SL'), ('성능', 'NNG'), ('테스트', 'NNG'), ('를', 'JKO'), ('진행할', 'VV+ETM'), ('예정', 'NNG'), ('입니다', 'VCP+EF'), ('.', 'SF'), ('존나', 'MAG'), ('맛있', 'VA'), ('다', 'EC')]\n",
      "Komoran\n",
      "명사 : ['오늘', '저녁', '실증', '랩', '성능', '테스트', '진행', '예정', '존']\n",
      "형태소 : ['오늘', '저녁', '에', '실증', '랩', '에서', 'PC', '성능', '테스트', '를', '진행', '하', 'ㄹ', '예정', '이', 'ㅂ니다', '.', '존', '나', '맛있', '다']\n",
      "품사 : [('오늘', 'NNG'), ('저녁', 'NNG'), ('에', 'JKB'), ('실증', 'NNG'), ('랩', 'NNG'), ('에서', 'JKB'), ('PC', 'SL'), ('성능', 'NNP'), ('테스트', 'NNP'), ('를', 'JKO'), ('진행', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETM'), ('예정', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF'), ('존', 'NNP'), ('나', 'NP'), ('맛있', 'VA'), ('다', 'EC')]\n"
     ]
    }
   ],
   "source": [
    "### KoNlpy의 Okt 형태소 분석기\n",
    "from konlpy.tag import Okt\n",
    "import konlpy.tag as tag\n",
    "okt = Okt()\n",
    "\n",
    "text = \"오늘 저녁에 실증랩에서 PC 성능 테스트를 진행할 예정입니다. 존나맛있다\"\n",
    "\n",
    "for okt in [tag.Okt(), tag.Kkma(), tag.Mecab(), tag.Komoran()]:\n",
    "    print(okt.__class__.__name__)\n",
    "    print(f\"명사 : {okt.nouns(text)}\")\n",
    "    print(f\"형태소 : {okt.morphs(text)}\")\n",
    "    print(f\"품사 : {okt.pos(text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nltk\n",
    "- 한글 미지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happy', 'New', 'tokens', '=', 'tokenize.word_tokenize', '(', 'text', ')']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "\n",
    "text = \"Happy New tokens = tokenize.word_tokenize(text)\"\n",
    "tokenize.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Happy',\n",
       " 'New',\n",
       " 'tokens',\n",
       " '=',\n",
       " 'tokenize',\n",
       " '.',\n",
       " 'word_tokenize',\n",
       " '(',\n",
       " 'text',\n",
       " ')']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctokenize = tokenize.WordPunctTokenizer()\n",
    "punctokenize.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 파이선 string 모듈에서 제공하는 구두점\n",
    "import string\n",
    "string.punctuation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
