{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn  \n",
    "import torch\n",
    "from Korpora import korpus_nsmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Korpora] Corpus `nsmc` is already installed at /media/data/KMS/KDT5_NL_Practice/DAY04/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /media/data/KMS/KDT5_NL_Practice/DAY04/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "korpus_nsmc.fetch_nsmc('./', force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size : 150000\n",
      "Test Data Size : 50000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "traindf =  pd.read_csv('./nsmc/ratings_train.txt', sep='\\t')\n",
    "testdf = pd.read_csv('./nsmc/ratings_test.txt', sep='\\t')\n",
    "\n",
    "print(\"Training Data Size :\", len(traindf))\n",
    "print(\"Test Data Size :\", len(testdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.dropna(inplace=True), testdf.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'document', 'label'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149995, 3), (49997, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.shape, testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [2] 단어 사전 생성\n",
    "<hr>\n",
    "\n",
    "* 토큰화 진행 ==> 형태소 분석기 선택\n",
    "* 단어 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n",
      "['<pad>', '<unk>', '.', '이', '는', '영화', '다', '고', '하', '도']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocab(corpus, n_vocab, special_tokens):\n",
    "    counter = Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab = special_tokens + [token for token, _ in counter.most_common(n_vocab)]\n",
    "\n",
    "    return vocab\n",
    "\n",
    "tokenizer = Mecab()\n",
    "train_tokens = [tokenizer.morphs(sentence) for sentence in traindf['document']]\n",
    "test_tokens = [tokenizer.morphs(sentence) for sentence in testdf['document']]\n",
    "\n",
    "vocab = build_vocab(train_tokens, n_vocab=10000, special_tokens=['<pad>', '<unk>'])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
    "\n",
    "print(len(vocab))\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] 데이터 가공\n",
    "<hr>\n",
    "\n",
    "- 토큰 데이터 정수 인코딩\n",
    "- 데이터 길이 표준화 => 다른 길이의 데이터를 길이 맞추기 -> 1개 문장 구성하는 단어 수 맞추기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3-1] 토큰 정수화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3-2] 데이터 구성 단어 수 맞추기 즉, 패딩(padding)\n",
    "- 단어 수 선정 필요\n",
    "- 선전된 단어 수 맞게 데이터 조절 => 길면 잘라내기, 짧으면 채우기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 41  86 958   2   2  48 245  28  42 765   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 토큰화할 데이터\n",
    "# 최대문장길이\n",
    "# 패딩처리시 값\n",
    "# 오른쪽 왼쭉 여부\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_val, trucateEnd = True):\n",
    "    import math\n",
    "    result = list()\n",
    "\n",
    "    for sequence in sequences:\n",
    "        pad_length = max_length - len(sequence)\n",
    "        sequence = sequence[:max_length]\n",
    "        paded_sequence = sequence + [pad_val] * pad_length\n",
    "        result.append(paded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id = token_to_id['<unk>']\n",
    "\n",
    "# 언어 단어에 없는 키워드의 경우에는 해당 값으로 변경될 수 있도로 dict.get(원단어, 원단어실패시기본<unk>)\n",
    "train_id = [\n",
    "        [token_to_id.get(token, unk_id) for token in tokens] for tokens in train_tokens\n",
    "]\n",
    "\n",
    "test_id = [\n",
    "    [token_to_id.get(token, unk_id) for token in tokens] for tokens in test_tokens\n",
    "]\n",
    "\n",
    "# 학습용, 테스트용 데이터 패딩 처리\n",
    "max_length = 32\n",
    "pad_id = token_to_id['<pad>']\n",
    "train_id = pad_sequences(train_id, max_length, pad_id)\n",
    "test_id = pad_sequences(test_id, max_length, pad_id)\n",
    "\n",
    "print(train_id[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] 데이터 학습 준비\n",
    "<hr>\n",
    "\n",
    "- 데이터로더 준비\n",
    "- 학습용/테스트용 함수\n",
    "- 모델 클래스\n",
    "- 학습 관련 변수 => DEVICE, Optimizer, Model, Loss Function, EPOCHS, BATCH SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4-1] 데이터 로더 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "train_id = torch.tensor(train_id)\n",
    "test_id = torch.tensor(test_id)\n",
    "\n",
    "train_labels = torch.tensor(traindf['label'].values, dtype=torch.float32)\n",
    "test_labels = torch.tensor(testdf['label'].values, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_id, train_labels)\n",
    "test_dataset = TensorDataset(test_id, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12812, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=12812, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4-2] 모델 클래스 정의\n",
    "\n",
    "- 입력층 : Embedding Layer\n",
    "- 은닉층 : LSTM Layer\n",
    "- 은닉층 : Dropout Layer\n",
    "- 출력층 : Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_vocab,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout = 0,\n",
    "            bidirectional = True,\n",
    "            model_type = 'lstm',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "\n",
    "        if model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.model(embedded)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        return self.classifier(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "\n",
    "classifier = SentenceClassifier(\n",
    "    n_vocab,\n",
    "    hidden_dim,\n",
    "    embedding_dim,\n",
    "    n_layers,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss : 0.6920441836118698, Test Accuracy : 0.5202912174730484\n",
      "Test loss : 0.6793728321790695, Test Accuracy : 0.6080964857891473\n",
      "Test loss : 0.595001757144928, Test Accuracy : 0.7051023061383683\n",
      "Test loss : 0.535707414150238, Test Accuracy : 0.7433245994759685\n",
      "Test loss : 0.48560822755098343, Test Accuracy : 0.7708462507750465\n",
      "Test loss : 0.4475458338856697, Test Accuracy : 0.7942276536592195\n",
      "Test loss : 0.41683295369148254, Test Accuracy : 0.8096285777146629\n",
      "Test loss : 0.3950471132993698, Test Accuracy : 0.8207492449546973\n",
      "Test loss : 0.3831527754664421, Test Accuracy : 0.8277496649798988\n",
      "Test loss : 0.3730240985751152, Test Accuracy : 0.833109986599196\n",
      "Test loss : 0.3689783066511154, Test Accuracy : 0.8373702422145328\n",
      "Test loss : 0.36283884197473526, Test Accuracy : 0.837750265015901\n",
      "Test loss : 0.3596683219075203, Test Accuracy : 0.8428905734344061\n",
      "Test loss : 0.35472220182418823, Test Accuracy : 0.8449306958417505\n",
      "Test loss : 0.35504648834466934, Test Accuracy : 0.8460907654459268\n",
      "Test loss : 0.35255619138479233, Test Accuracy : 0.8481308878532712\n",
      "Test loss : 0.36130841821432114, Test Accuracy : 0.8461107666459987\n",
      "Test loss : 0.35701417922973633, Test Accuracy : 0.8494109646578795\n",
      "Test loss : 0.36805394291877747, Test Accuracy : 0.8453107186431186\n",
      "Test loss : 0.3606438934803009, Test Accuracy : 0.8500310018601116\n"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if step % interval == 0:\n",
    "        #     print(f'Train loss {step} : {np.mean(losses)}')\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    corrects = []\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits) >.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).tolist()\n",
    "        )\n",
    "\n",
    "    print(f'Test loss : {np.mean(losses)}, Test Accuracy : {np.mean(corrects)}')\n",
    "\n",
    "\n",
    "epoches = 20\n",
    "interval = 100\n",
    "\n",
    "for epoch in range(1, epoches + 1):\n",
    "    train(classifier, train_loader, criterion, optimizer, DEVICE, interval)\n",
    "    test(classifier, test_loader, criterion, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
